# Pyspark_preparation_guide
This repository guides an individual preparing pyspark for interviews. 

Overview:

Here are the top, most essential functions in each of the three categories for day-to-day PySpark development.1. DataFrame Methods (df.)Structural "Verbs" used to shape the table.These are methods called directly on your DataFrame object (e.g., df.select(...)).Sub-CategoryFunctionDescriptionSelection & Structureselect()Choose specific columns to keep.withColumn()Create a new column or update an existing one.withColumnRenamed()Rename an existing column.drop()Remove a column from the DataFrame.printSchema()Print the column names and data types (debugging).Filtering & Sortingfilter() / where()Filter rows based on a condition (SQL style or logic).orderBy() / sort()Sort rows by specific columns.distinct()Remove duplicate rows.limit()Restrict the output to the top N rows.Grouping & JoininggroupBy()Group data (followed by an aggregate function).join()Merge two DataFrames based on a key.union() / unionByName()Append rows from one DataFrame to another.Actions (Triggers)show() / display()View the data (triggers computation).count()Return the number of rows.writeSave data (e.g., df.write.parquet(...)).2. Column Functions (F.)Logic "Adjectives" used inside DataFrame methods.You must import these: from pyspark.sql import functions as F.Sub-CategoryFunctionDescriptionBasicscol()Refers to a column (essential for complex logic).lit()Creates a literal (constant) value.expr()Allows you to write raw SQL snippets as code.Conditionalwhen().otherwise()The if-then-else logic of Spark.coalesce()Returns the first non-null value in a list of columns.Aggregationsum(), avg(), mean()Basic math statistics.min(), max()Finds boundaries of data.count(), countDistinct()Counts rows or unique values.String Opsconcat(), concat_ws()Joins strings together.substring()Extracts part of a string.upper(), lower(), trim()Formatting text.split()Splits a string into an array.Date & Timecurrent_date()Gets today's date.to_date(), to_timestamp()Converts strings to date/time objects.date_add(), datediff()Adds days or finds difference between dates.Window-Specificrow_number()Assigns a unique ID to rows in a window.rank(), dense_rank()Ranking (dense = no gaps in rank numbers).lead(), lag()Gets value from next row or previous row.Complex Typesexplode()Explodes an array column into multiple rows (one per item).struct()Combines columns into a nested object.3. Window Specifications (Window.)Scope Definitions for context.You must import this: from pyspark.sql import Window.Note: This category is small because it only has one jobâ€”defining the "frame."FunctionDescriptionpartitionBy()The Grouper. Splits the data into logical groups (like groupBy, but keeps rows).orderBy()The Sorter. Determines the order of rows within the partition (crucial for ranking).rowsBetween()The Slider (Physical). Looks at physical rows relative to current (e.g., "previous 1 row").rangeBetween()The Slider (Logical). Looks at value ranges relative to current (e.g., "dates within 7 days").unboundedPrecedingBoundary. Start at the very first row of the partition.unboundedFollowingBoundary. End at the very last row of the partition.Top 3 "Power Combos"Here is how developers combine these three categories in real scripts:The "Feature Engineer"df.withColumn() + F.when() + F.col()Used to create new flags or categories based on logic.The "Deduplicator"Window.partitionBy() + F.row_number() + df.filter()Used to keep only the "most recent" record per user.The "Time Traveler"Window.partitionBy() + F.lag() + df.withColumn()Used to compare today's data with yesterday's data.


then moves on to practical questions then deep dives into pyspark performance optimization (Data Layout Optimization) through partitionBy, Bucketing and Salting, Z-Ordering, then to building spark clusters.
